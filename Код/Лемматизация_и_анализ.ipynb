{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TxYD3kX-fQSK"
      },
      "outputs": [],
      "source": [
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "VFThJP_WvNXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "kMOAuTeLC326"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "CYsSk-ArjoNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "gbjllIcvu7Jy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "LoGS6X1QsBAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy==3.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K658p9Ugu7_C",
        "outputId": "96b5ef93-ecc4-4fd4-93f9-7f1051df7628"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting spacy==3.2\n",
            "  Downloading spacy-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.9/dist-packages (from spacy==3.2) (0.10.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from spacy==3.2) (23.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from spacy==3.2) (3.1.2)\n",
            "Collecting wasabi<1.1.0,>=0.8.1\n",
            "  Downloading wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from spacy==3.2) (0.7.9)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.9/dist-packages (from spacy==3.2) (1.0.9)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
            "  Downloading pydantic-1.8.2-cp39-cp39-manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typer<0.5.0,>=0.3.0\n",
            "  Downloading typer-0.4.2-py3-none-any.whl (27 kB)\n",
            "Collecting thinc<8.1.0,>=8.0.12\n",
            "  Downloading thinc-8.0.17-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (668 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m668.8/668.8 kB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from spacy==3.2) (67.7.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.9/dist-packages (from spacy==3.2) (2.4.6)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.9/dist-packages (from spacy==3.2) (1.24.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/dist-packages (from spacy==3.2) (2.27.1)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from spacy==3.2) (1.0.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.9/dist-packages (from spacy==3.2) (4.65.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy==3.2) (2.0.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.9/dist-packages (from spacy==3.2) (2.0.8)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.9/dist-packages (from spacy==3.2) (3.0.12)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from spacy==3.2) (3.3.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy==3.2) (3.0.8)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.9/dist-packages (from pathy>=0.3.5->spacy==3.2) (6.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy==3.2) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.2) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.2) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.2) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.2) (3.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.9/dist-packages (from typer<0.5.0,>=0.3.0->spacy==3.2) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->spacy==3.2) (2.1.2)\n",
            "Installing collected packages: wasabi, typer, pydantic, thinc, spacy\n",
            "  Attempting uninstall: wasabi\n",
            "    Found existing installation: wasabi 1.1.1\n",
            "    Uninstalling wasabi-1.1.1:\n",
            "      Successfully uninstalled wasabi-1.1.1\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.7.0\n",
            "    Uninstalling typer-0.7.0:\n",
            "      Successfully uninstalled typer-0.7.0\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 1.10.7\n",
            "    Uninstalling pydantic-1.10.7:\n",
            "      Successfully uninstalled pydantic-1.10.7\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.1.9\n",
            "    Uninstalling thinc-8.1.9:\n",
            "      Successfully uninstalled thinc-8.1.9\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.5.2\n",
            "    Uninstalling spacy-3.5.2:\n",
            "      Successfully uninstalled spacy-3.5.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "inflect 6.0.4 requires pydantic>=1.9.1, but you have pydantic 1.8.2 which is incompatible.\n",
            "en-core-web-sm 3.5.0 requires spacy<3.6.0,>=3.5.0, but you have spacy 3.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pydantic-1.8.2 spacy-3.2.0 thinc-8.0.17 typer-0.4.2 wasabi-0.10.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spacy.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "qBDlpc0du98B",
        "outputId": "7c70c789-e5ae-498f-f92b-f80ff0669511"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3.5.2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "EQTYCrLeu-rr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download ru_core_news_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y17t2LBvu-98",
        "outputId": "1085e6c6-12f7-43b6-ef67-ae62f0e254d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-04-26 15:36:33.177644: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[33mDEPRECATION: https://github.com/explosion/spacy-models/releases/download/ru_core_news_sm-3.2.0/ru_core_news_sm-3.2.0-py3-none-any.whl#egg=ru_core_news_sm==3.2.0 contains an egg fragment with a non-PEP 508 name pip 25.0 will enforce this behaviour change. A possible replacement is to use the req @ url syntax, and remove the egg fragment. Discussion can be found at https://github.com/pypa/pip/issues/11617\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ru-core-news-sm==3.2.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/ru_core_news_sm-3.2.0/ru_core_news_sm-3.2.0-py3-none-any.whl (16.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pymorphy2>=0.9\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from ru-core-news-sm==3.2.0) (3.2.0)\n",
            "Collecting pymorphy2-dicts-ru<3.0,>=2.4\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docopt>=0.6\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting dawg-python>=0.7.1\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.3.0,>=3.2.0->ru-core-news-sm==3.2.0) (1.0.9)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.9/dist-packages (from spacy<3.3.0,>=3.2.0->ru-core-news-sm==3.2.0) (1.8.2)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.3.0,>=3.2.0->ru-core-news-sm==3.2.0) (0.4.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.3.0,>=3.2.0->ru-core-news-sm==3.2.0) (2.0.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.9/dist-packages (from spacy<3.3.0,>=3.2.0->ru-core-news-sm==3.2.0) (3.0.12)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.3.0,>=3.2.0->ru-core-news-sm==3.2.0) (0.7.9)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.3.0,>=3.2.0->ru-core-news-sm==3.2.0) (4.65.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from spacy<3.3.0,>=3.2.0->ru-core-news-sm==3.2.0) (67.7.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.3.0,>=3.2.0->ru-core-news-sm==3.2.0) (3.3.0)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.9/dist-packages (from spacy<3.3.0,>=3.2.0->ru-core-news-sm==3.2.0) (8.0.17)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.9/dist-packages (from spacy<3.3.0,>=3.2.0->ru-core-news-sm==3.2.0) (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.3.0,>=3.2.0->ru-core-news-sm==3.2.0) (1.24.3)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.9/dist-packages (from spacy<3.3.0,>=3.2.0->ru-core-news-sm==3.2.0) (2.0.8)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.9/dist-packages (from spacy<3.3.0,>=3.2.0->ru-core-news-sm==3.2.0) (2.4.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.9/dist-packages (from spacy<3.3.0,>=3.2.0->ru-core-news-sm==3.2.0) (0.10.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.3.0,>=3.2.0->ru-core-news-sm==3.2.0) (3.1.2)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.3.0,>=3.2.0->ru-core-news-sm==3.2.0) (1.0.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.3.0,>=3.2.0->ru-core-news-sm==3.2.0) (3.0.8)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.3.0,>=3.2.0->ru-core-news-sm==3.2.0) (2.27.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.3.0,>=3.2.0->ru-core-news-sm==3.2.0) (23.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.9/dist-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->ru-core-news-sm==3.2.0) (6.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->ru-core-news-sm==3.2.0) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->ru-core-news-sm==3.2.0) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->ru-core-news-sm==3.2.0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->ru-core-news-sm==3.2.0) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->ru-core-news-sm==3.2.0) (1.26.15)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.9/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->ru-core-news-sm==3.2.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->spacy<3.3.0,>=3.2.0->ru-core-news-sm==3.2.0) (2.1.2)\n",
            "Building wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13721 sha256=3f033660c4f348827e07818c9bbef3145bfff91be5588b91b097f21b9794da0f\n",
            "  Stored in directory: /root/.cache/pip/wheels/70/4a/46/1309fc853b8d395e60bafaf1b6df7845bdd82c95fd59dd8d2b\n",
            "Successfully built docopt\n",
            "Installing collected packages: pymorphy2-dicts-ru, docopt, dawg-python, pymorphy2, ru-core-news-sm\n",
            "Successfully installed dawg-python-0.7.2 docopt-0.6.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844 ru-core-news-sm-3.2.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('ru_core_news_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('ru_core_news_sm')"
      ],
      "metadata": {
        "id": "6-9gQmGGu_8h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9df90ee4-b36a-4067-c02d-f7d3ab9c2a9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/spacy/util.py:887: UserWarning: [W095] Model 'ru_core_news_sm' (3.2.0) was trained with spaCy v3.2 and may not be 100% compatible with the current version (3.5.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
            "  \"\"\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.max_length = 97404456"
      ],
      "metadata": {
        "id": "_BGZwaiW0-RK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwordsspacy = nlp.Defaults.stop_words"
      ],
      "metadata": {
        "id": "eRxcuOCEvEIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "noun_magazines = []\n",
        "verb_magazines = []\n",
        "adj_magazines = []"
      ],
      "metadata": {
        "id": "6tYiXNAtKlA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dirpath = 'magazines_pre_add'\n",
        "for filename in os.listdir(dirpath):\n",
        "  filepath = os.path.join(dirpath, filename)\n",
        "  if os.path.isfile(filepath):\n",
        "    try:\n",
        "      with open(filepath, encoding = 'utf-8') as fd:\n",
        "        text = fd.read()\n",
        "      text = text.lower()\n",
        "      textforspacy = nlp(text)\n",
        "      for token in textforspacy:\n",
        "        if token.lemma_ not in stopwordsspacy:\n",
        "          if token.tag_ == 'NOUN':\n",
        "            noun_magazines.append(token.lemma_)\n",
        "          elif token.tag_ == 'ADJ':\n",
        "            adj_magazines.append(token.lemma_)\n",
        "          elif token.tag_ == 'VERB':\n",
        "            verb_magazines.append(token.lemma_)\n",
        "      print(filename)                           \n",
        "    except:\n",
        "      print(f'С файлом {filename} проблемка')    "
      ],
      "metadata": {
        "id": "UJGqVETC-4-2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d260182-1e40-46f9-ff53-aa0bae619eeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ogonek_pre_add_2_1.txt\n",
            "ogonek_pre_add_2.txt\n",
            "ogonek_pre_add_3.txt\n",
            "ogonek_pre_add_1.txt\n",
            "ogonek_pre_add_2_2.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "noun_data_magazines = pd.Series(noun_magazines)\n",
        "verb_data_magazines = pd.Series(verb_magazines)\n",
        "adj_data_magazines = pd.Series(adj_magazines)"
      ],
      "metadata": {
        "id": "1HSd4pJDceNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "noun_data_magazines.to_excel('noun_data_magazines_pre_add.xlsx') \n",
        "verb_data_magazines.to_excel('verb_data_magazines_pre_add.xlsx') \n",
        "adj_data_magazines.to_excel('adj_data_magazines_pre_add.xlsx') "
      ],
      "metadata": {
        "id": "I_g7We1jcgIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dirpath = 'magazines_pre_4'\n",
        "for filename in os.listdir(dirpath):\n",
        "  filepath = os.path.join(dirpath, filename)\n",
        "  if os.path.isfile(filepath):\n",
        "    try:\n",
        "      with open(filepath, encoding = 'utf-8') as fd:\n",
        "        text = fd.read()\n",
        "      text = text.lower()\n",
        "      textforspacy = nlp(text)\n",
        "      for token in textforspacy:\n",
        "        if token.lemma_ not in stopwordsspacy:\n",
        "          if token.tag_ == 'NOUN':\n",
        "            noun_magazines.append(token.lemma_)\n",
        "          elif token.tag_ == 'ADJ':\n",
        "            adj_magazines.append(token.lemma_)\n",
        "          elif token.tag_ == 'VERB':\n",
        "            verb_magazines.append(token.lemma_)\n",
        "      print(filename)                           \n",
        "    except:\n",
        "      print(f'С файлом {filename} проблемка')    "
      ],
      "metadata": {
        "id": "8SwlKf9pyS0O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24dad8c6-0bbb-4234-fb69-6ddc6978430d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "molodjoz-estonii.txt\n",
            "moskovskaya_pravda_1971.txt\n",
            "krokodil_1965-1984_чистое.txt\n",
            "mk_1971.txt\n",
            "mayak_severa_1976.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dirpath = 'magazines_pre_5'\n",
        "for filename in os.listdir(dirpath):\n",
        "  filepath = os.path.join(dirpath, filename)\n",
        "  if os.path.isfile(filepath):\n",
        "    try:\n",
        "      with open(filepath, encoding = 'utf-8') as fd:\n",
        "        text = fd.read()\n",
        "      text = text.lower()\n",
        "      textforspacy = nlp(text)\n",
        "      for token in textforspacy:\n",
        "        if token.lemma_ not in stopwordsspacy:\n",
        "          if token.tag_ == 'NOUN':\n",
        "            noun_magazines.append(token.lemma_)\n",
        "          elif token.tag_ == 'ADJ':\n",
        "            adj_magazines.append(token.lemma_)\n",
        "          elif token.tag_ == 'VERB':\n",
        "            verb_magazines.append(token.lemma_)\n",
        "      print(filename)                           \n",
        "    except:\n",
        "      print(f'С файлом {filename} проблемка')    "
      ],
      "metadata": {
        "id": "vrqQOhW-9BtY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af302bea-0f03-4b08-c209-005f2da24ebe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ogonek_pre_1_1.txt\n",
            "ogonek_pre_3.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "noun_literature = []\n",
        "verb_literature = []\n",
        "adj_literature = []"
      ],
      "metadata": {
        "id": "F5N8Iszsj-Gt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dirpath = 'literature_perestroika' \n",
        "for filename in os.listdir(dirpath):\n",
        "  filepath = os.path.join(dirpath, filename)\n",
        "  if os.path.isfile(filepath):\n",
        "    try:\n",
        "      with open(filepath, encoding = 'utf-8') as fd:\n",
        "        text = fd.read()\n",
        "      text = text.lower()\n",
        "      textforspacy = nlp(text)\n",
        "      for token in textforspacy:\n",
        "        if token.lemma_ not in stopwordsspacy:\n",
        "          if token.tag_ == 'ADJ':\n",
        "            adj_literature.append(token.lemma_)\n",
        "          elif token.tag_ == 'VERB':\n",
        "            verb_literature.append(token.lemma_)  \n",
        "          elif token.tag_ == 'NOUN':\n",
        "            noun_literature.append(token.lemma_)   \n",
        "      print(filename)\n",
        "    except:\n",
        "      print(f'С файлом {filename} проблемка')      "
      ],
      "metadata": {
        "id": "Q90kzeAb_ro1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "noun_data_perestroika_literature = pd.Series(noun_literature)\n",
        "verb_data_perestroika_literature = pd.Series(verb_literature)\n",
        "adj_data_perestroika_literature = pd.Series(adj_literature)"
      ],
      "metadata": {
        "id": "CjR_GpiPj322"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "noun_data_perestroika_literature.to_excel('noun_data_perestroika_literature.xlsx') \n",
        "verb_data_perestroika_literature.to_excel('verb_data_perestroika_literature.xlsx') \n",
        "adj_data_perestroika_literature.to_excel('adj_data_perestroika_literature.xlsx') "
      ],
      "metadata": {
        "id": "k4HPjnwkj4Ya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "noun_speeches = []\n",
        "verb_speeches = []\n",
        "adj_speeches = []"
      ],
      "metadata": {
        "id": "8RLiiwqskWV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dirpath = 'speeches_perestroika' \n",
        "for filename in os.listdir(dirpath):\n",
        "  filepath = os.path.join(dirpath, filename)\n",
        "  if os.path.isfile(filepath):\n",
        "    try:\n",
        "      with open(filepath, encoding = 'utf-8') as fd:\n",
        "        text = fd.read()\n",
        "      text = text.lower()\n",
        "      textforspacy = nlp(text)\n",
        "      for token in textforspacy:\n",
        "        if token.lemma_ not in stopwordsspacy:\n",
        "          if token.tag_ == 'ADJ':\n",
        "            adj_speeches.append(token.lemma_)\n",
        "          elif token.tag_ == 'VERB':\n",
        "            verb_speeches.append(token.lemma_)  \n",
        "          elif token.tag_ == 'NOUN':\n",
        "            noun_speeches.append(token.lemma_)   \n",
        "      print(filename)\n",
        "    except:\n",
        "      print(f'С файлом {filename} проблемка') "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBmcJnt840U6",
        "outputId": "affb6bd7-fca1-4ea3-9116-ca8e8cf35f5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speeches_perestroika_2.txt\n",
            "speeches_perestroika.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "noun_data_perestroika_speeches = pd.Series(noun_speeches)\n",
        "verb_data_perestroika_speeches = pd.Series(verb_speeches)\n",
        "adj_data_perestroika_speeches = pd.Series(adj_speeches)"
      ],
      "metadata": {
        "id": "p2G0mvwXj5_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "noun_data_perestroika_speeches.to_excel('noun_data_perestroika_speeches.xlsx') \n",
        "verb_data_perestroika_speeches.to_excel('verb_data_perestroika_speeches.xlsx') \n",
        "adj_data_perestroika_speeches.to_excel('adj_data_perestroika_speeches.xlsx') "
      ],
      "metadata": {
        "id": "H98979-MkL1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "noun_subtitles = []\n",
        "verb_subtitles = []\n",
        "adj_subtitles = []"
      ],
      "metadata": {
        "id": "eu88o9b0ki_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('subtitles_perestroika.txt', encoding = 'utf-8') as fd:\n",
        "  text = fd.read()\n",
        "text = text.lower()\n",
        "textforspacy = nlp(text)\n",
        "for token in textforspacy:\n",
        "  if token.lemma_ not in stopwordsspacy:\n",
        "    if token.tag_ == 'NOUN':\n",
        "      noun_subtitles.append(token.lemma_)\n",
        "    elif token.tag_ == 'ADJ':\n",
        "      adj_subtitles.append(token.lemma_)\n",
        "    elif token.tag_ == 'VERB':\n",
        "      verb_subtitles.append(token.lemma_)"
      ],
      "metadata": {
        "id": "9_TqyS9XE1id"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "noun_data_perestroika_subtitles = pd.Series(noun_subtitles)\n",
        "verb_data_perestroika_subtitles = pd.Series(verb_subtitles)\n",
        "adj_data_perestroika_subtitles = pd.Series(adj_subtitles)"
      ],
      "metadata": {
        "id": "DaBUAkHpwEDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "noun_data_perestroika_subtitles.to_excel('noun_data_perestroika_subtitles.xlsx') \n",
        "verb_data_perestroika_subtitles.to_excel('verb_data_perestroika_subtitles.xlsx') \n",
        "adj_data_perestroika_subtitles.to_excel('adj_data_perestroika_subtitles.xlsx') "
      ],
      "metadata": {
        "id": "JnhLBW7hFbbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PyMorphy - почистим списки**"
      ],
      "metadata": {
        "id": "I4T5UA54Hs1f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymorphy2"
      ],
      "metadata": {
        "id": "UUMjeHS0H1YU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pymorphy2 import MorphAnalyzer\n",
        "morph = MorphAnalyzer()"
      ],
      "metadata": {
        "id": "fdo8OPLVJlnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "morph.parse('мыло')[0].normal_form"
      ],
      "metadata": {
        "id": "-flZkh8MJm5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "morph.parse('мыло')[0].tag"
      ],
      "metadata": {
        "id": "iaXGayI0JyJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "noun_data = pd.read_excel('noun.xslx')\n",
        "verb_data = pd.read_excel('verb.xslx')\n",
        "adj_data = pd.read_excel('adj.xslx')"
      ],
      "metadata": {
        "id": "QOVYoNSEKq8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "noun_list = noun_data['Слово'].to_list()\n",
        "verb_list = verb_data['Слово'].to_list()\n",
        "adj_list = adj_data['Слово'].to_list()"
      ],
      "metadata": {
        "id": "CAdJN1PbK4Hn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "noun_perestroika_final = []\n",
        "verb_perestroika_final = []\n",
        "adj_perestroika_final = []"
      ],
      "metadata": {
        "id": "fw-xy6vMKZpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in noun_list:\n",
        "    normal_form = morph.parse(word)[0].normal_form\n",
        "    noun_perestroika_final.append(normal_form)\n",
        "for word in verb_list:\n",
        "    normal_form = morph.parse(word)[0].normal_form\n",
        "    verb_perestroika_final.append(normal_form)\n",
        "for word in adj_list:\n",
        "    normal_form = morph.parse(word)[0].normal_form\n",
        "    adj_perestroika_final.append(normal_form)        "
      ],
      "metadata": {
        "id": "fJyPp-XfKIs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "noun_data_perestroika_clean = pd.Series(noun_perestroika_final)\n",
        "verb_data_perestroika_clean = pd.Series(verb_perestroika_final)\n",
        "adj_data_perestroika_clean = pd.Series(adj_perestroika_final)"
      ],
      "metadata": {
        "id": "LO6ou5ngQC-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "noun_data_perestroika_clean.to_csv('noun_data_perestroika_clean.csv', encoding = 'utf-8', index = False)\n",
        "verb_data_perestroika_clean.to_csv('verb_data_perestroika_clean.csv', encoding = 'utf-8', index = False)\n",
        "adj_data_perestroika_clean.to_csv('adj_data_perestroika_clean.csv', encoding = 'utf-8', index = False)"
      ],
      "metadata": {
        "id": "enYekMZsQMpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "noun_data_perestroika_clean.to_excel('noun_data_perestroika_clean.xlsx') \n",
        "verb_data_perestroika_clean.to_excel('verb_data_perestroika_clean.xlsx') \n",
        "adj_data_perestroika_clean.to_excel('adj_data_perestroika_clean.xlsx') "
      ],
      "metadata": {
        "id": "lCPFfvjoQNjq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}